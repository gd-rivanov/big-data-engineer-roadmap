# Big data T1 (Scala)

## English
-  pre-intermediate
-  listening
-  writing


## Client Communication 
-  daily status
-  client interview
-  tech discussion (with mentor)


## Agile
-  kanban vs scrum
-  active participation


## Workflow Management
- Basic experience with any orchestrator (Airflow/ Azkaban / NiFi / cron etc )
  
  
## Issue Tracking
-  Understanding of typical workflow of issue
-  Create UserStory (with mentor)


## VCS
-  Branch management
-  Git


## Data Warehouse
### Architecture
- CAP theorem

### Database fundamentals
- SQL 
- ACID transactions
- Normalisation


## Big Data
- Batch vs streaming processing (basic concepts)
- Distributed storage (HDFS, MinIO etc)  
- Distributed calculation and engines
  - MapReduce concept
  - Experience with any engine (Spark/Flume/Presto/HiveOnTez/Dremio etc) 
- (nice to have) Experience with queues (Kafka/ Kinesis/ Streams/ RabbitMQ etc)
- (nice to have) Notebooks (Jupiter/ Zeppelin)


## Programming
### CS Fundamentals
- Data Structures and Algorithms, big O notation
- Basic level in software design (OOP, FP, Code quality, Modularity, GOF, SOLID etc)
- Basic terminal usage
- Linux
  - Shell scripting (Bash)
  - Text editors (vim etc)
  - Work with remote instances (ssh/scp/screen etc)
- Refactoring
- Working with IDE (Idea)
   
### JVM background
- Packaging/dependencies/repositories
- Build lifecycle
- SBT/Maven/Gradle  

### Scala
- Scala core (types, collections etc)
- Build package(sbt/maven/gradle)
- (nice to have) Feature implementation (traites/options/etc)
- Scala libs (spark/akka/cats/etc)
- Test libs

### Python (nice to have)
- Scripting/Procedural programming level
- Package management (pip/conda etc)
- Work with virtual environment

### Testing
- Unit tests
- Test life cycle

## Clouds (nice to have) 
- Cloud certification (recommended)
- DataWarehouse design in clouds
- DataLake design in clouds
- Workflow orchestration
- SDK/API and other integrations with DBs/clouds