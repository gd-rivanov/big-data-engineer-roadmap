# Big data T1 (Python)

## English
-  pre-intermediate
-  listening
-  writing


## Client Communication 
-  daily status
-  client interview
-  tech discussion (with mentor)


## Agile
-  kanban vs scrum
-  active participation


## Workflow Management
- Basic experience with any orchestrator (Airflow/ Azkaban / NiFi / cron etc )
  
  
## Issue Tracking
-  Understanding of typical workflow of issue
-  Create UserStory (with mentor)


## VCS
-  Branch management
-  Git


## Data Warehouse
### Architecture
- CAP theorem

### Database fundamentals
- SQL 
- ACID transactions
- Normalisation


## Big Data
- Batch vs streaming processing (basic concepts)
- Distributed storage (HDFS, MinIO etc)  
- Distributed calculation and engines
  - MapReduce concept
  - Experience with any engine (Spark/Flume/Presto/HiveOnTez/Dremio etc) 
- (nice to have) Experience with queues (Kafka/ Kinesis/ Streams / RabbitMQ etc)
- Notebooks (Jupiter/ Zeppelin)


## Programming
### CS Fundamentals
- Data Structures and Algorithms, big O notation
- Basic level in software design (OOP, FP, Code quality, Modularity, GOF, SOLID etc)
- Basic terminal usage
- Linux
  - Shell scripting (Bash)
  - Text editors (vim etc)
  - Work with remote instances (ssh/scp/screen etc)
- Refactoring
- Working with IDE (PyCharm)  
  
### JVM background (theory) (nice to have)
- Packaging/dependencies/repositories (maven/sbt/gradle)

### Python
- Python core (data types, collections etc)
- multithreading (theory)
- Python libs (PySpark/pandas/numpy etc)
- Package management (pip/conda etc)
- Work with virtual environments
- Test libs

### Testing
- Unit tests
- Test life cycle

## Clouds (nice to have) 
- Cloud certification (recommended)
- DataWarehouse design in clouds
- DataLake design in clouds
- Workflow orchestration
- SDK/API and other integrations with DBs/clouds