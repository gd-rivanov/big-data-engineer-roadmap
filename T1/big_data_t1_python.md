# Big data T1 (Python)

## English
-  pre-intermediate
-  listening
-  writing

## Client Communication 
-  daily status
-  client interview
-  tech discussion (with mentor)

## Agile
-  process contribution
-  kanban vs scrum
-  active participation

## Workflow Management
- Experience with any orchestrator (Airflow/ Azkaban / NiFi / cron etc )
- (nice to have) Airflow (DAG, architecture, XCom)
  
## Issue Tracking
-  Understanding of typical workflow of issue
-  Create UserStory (with mentor)

## VCS
-  Code review
-  Branch management
-  Git


## Data Warehouse
### Architecture
- Dimensional modelling
- data layers (raw/stage/gold)
- architecture variance
- DataLake vs DataWarehouse vs Database
- CAP theorem

### Database fundamentals
- SQL 
- ACID transactions
- Normalisation


## Big Data
- Batch vs streaming processing
- Distributed storage (HDFS, MinIO etc)  
- Distributed calculation and engines
  - MapReduce pattern theory
  - Experience with any engine (Spark/Flume/Presto/HiveOnTez/Dremio etc) 
- (nice to have) Experience with distributed queue (Kafka/ Kinesis/ Streams etc)
- Notebooks (Jupiter/ Zeppelin)


## Programming
### CS Fundamentals
- Data Structures and Algorithms, big O notation
- Basic level in software design (OOP, FP, Code quality, Modularity, GOF, SOLID etc)
- Basic terminal usage
- Linux
  - Shell scripting (Bash)
  - Text editors (vim etc)
  - Work with remote instances (ssh/scp/screen etc)
  
### JVM (theory)

### Python
- Python core (data types, collections etc)
- multithreading (theory)
- Python libs (PySpark/pandas/numpy etc)
- Package management (pip/conda etc)
- Work with virtual environments
- Test libs

### Testing
- Unit tests